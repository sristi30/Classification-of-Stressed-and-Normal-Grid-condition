{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"HW_TESLA.xlt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1636, 133)\n",
      "(2514, 133)\n",
      "(4150, 133)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel (r'HW_TESLA.xlt') \n",
    "print(df.iloc[0:1636].shape) #label 0 data\n",
    "print(df.iloc[1636:].shape) #label 1 data\n",
    "print (df.shape)\n",
    "#train=df.iloc[1636:].sample(frac=0.75,random_state=200)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1038, 132)\n",
      "(1038,)\n",
      "(3112, 132)\n",
      "(3112,)\n"
     ]
    }
   ],
   "source": [
    "#splitting into train and test  and val\n",
    "train=df.sample(frac=0.75,random_state=200) #random state is a seed value\n",
    "test=df.drop(train.index)\n",
    "features = list(train.columns[1:])\n",
    "# x_train = train[features]\n",
    "# y_train = train['STATIC']\n",
    "x_test = test[features]\n",
    "y_test = test['STATIC']\n",
    "#train_y = y_train.values\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "x_train = train[features]\n",
    "y_train = train['STATIC']\n",
    "train_y = y_train.values\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative_train = train[train.STATIC == 0]\n",
    "# positive_train = train[train.STATIC == 1]\n",
    "# print(negative_train.shape)\n",
    "# print(positive_train.shape)\n",
    "# oversampled_negative = negative_train.sample(654, random_state=200)\n",
    "# print(oversampled_negative.shape)\n",
    "# #append oversampled data to original data\n",
    "# train = pd.concat([oversampled_negative, train], ignore_index=True)\n",
    "# print(train.shape)\n",
    "# x_train = train[features]\n",
    "# y_train = train['STATIC']\n",
    "# train_y = y_train.values\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3766, 5)\n",
      "(1038, 5)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=5)\n",
    "pca.fit(x_train)\n",
    "#transform the train data into principal components\n",
    "train_x = pca.transform(x_train)\n",
    "print(train_x.shape)\n",
    "# transform the test data into principal components\n",
    "test_x = pca.transform(x_test)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f1:  0.9877717423886931\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, random_state=200, shuffle=True)\n",
    "mean_f1 = 0\n",
    "# train_x = train_x.values # to convert df to numpy arrays\n",
    "\n",
    "#best_accuracy = 0\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    kx_train= train_x[train_index]\n",
    "    kx_test = train_x[test_index]\n",
    "    ky_train = train_y[train_index]\n",
    "    ky_test = train_y[test_index]\n",
    "    #-------------------------------------------para search by grid search\n",
    "#     param_grid = {'max_depth': np.arange(3, 6),'min_samples_split':np.arange(2,4),'min_samples_leaf':np.arange(1,4),\n",
    "#               'criterion':['gini','entropy']}\n",
    "#     tree = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "#     tree.fit(kx_train, ky_train)\n",
    "#     tree_preds = tree.predict_proba(kx_test)[:, 1]\n",
    "#     tree_performance = roc_auc_score(ky_test, tree_preds)\n",
    "#     print(tree.best_params_)\n",
    "    #------------------------------------------------------\n",
    "    # Decision tree with best parameters\n",
    "#     best_dp = tree.best_params_['max_depth']\n",
    "#     min_smple_split = tree.best_params_['min_samples_split']\n",
    "#     min_smple_lf = tree.best_params_['min_samples_leaf']\n",
    "#     best_crit = tree.best_params_['criterion']\n",
    "\n",
    "##################Check max depth arameter######################\n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit(kx_train, ky_train)\n",
    "    y_pred = dt.predict(kx_test)\n",
    "    #acc = metrics.accuracy_sc(ky_test, y_pred)\n",
    "    #print(acc)\n",
    "    f1 = f1_score(ky_test, y_pred)\n",
    "#     if(acc > best_accuracy):\n",
    "#         best_accuracy = acc\n",
    "#         with open('best_pca_dt.pkl', 'wb') as f:\n",
    "#             pickle.dump(dt, f)            \n",
    "    mean_f1 += f1\n",
    "mean_f1 /= 3\n",
    "print(\"Mean f1: \", mean_f1)\n",
    "\n",
    "\n",
    "\n",
    "# Load the pickled model \n",
    "# with open('best_pca_dt.pkl', 'rb') as f:\n",
    "#     model_from_pickle = pickle.load(f)\n",
    "# Use the loaded pickled model to make predictions \n",
    "#y_pred = dt.predict(test_x)\n",
    "# print(\"Validation Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "# print(tn,fp,fn,tp)\n",
    "# test_f1 = f1_score(y_test, y_pred)\n",
    "# print(\"Test f1 score: \", test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f1:  0.991336028459691\n"
     ]
    }
   ],
   "source": [
    "mean_f1 = 0\n",
    "\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    kx_train= train_x[train_index]\n",
    "    kx_test = train_x[test_index]\n",
    "    ky_train = train_y[train_index]\n",
    "    ky_test = train_y[test_index]\n",
    "    SVM = svm.SVC(kernel='linear')\n",
    "    SVM.fit(kx_train, ky_train)\n",
    "    y_pred = SVM.predict(kx_test)\n",
    "    f1 = f1_score(ky_test, y_pred)         \n",
    "    mean_f1 += f1\n",
    "mean_f1 /= 3\n",
    "print(\"Mean f1: \", mean_f1)\n",
    "# y_pred = dt.predict(test_x)\n",
    "# test_f1 = f1_score(y_test, y_pred)\n",
    "# print(\"Test f1 score: \", test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f1:  0.9830935176821239\n"
     ]
    }
   ],
   "source": [
    "mean_f1 = 0\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    kx_train= train_x[train_index]\n",
    "    kx_test = train_x[test_index]\n",
    "    ky_train = train_y[train_index]\n",
    "    ky_test = train_y[test_index]\n",
    "    RF = RandomForestClassifier(n_estimators=350, max_depth=5, random_state=99)\n",
    "    RF.fit(kx_train, ky_train)\n",
    "    y_pred = RF.predict(kx_test)\n",
    "    f1 = f1_score(ky_test, y_pred)         \n",
    "    mean_f1 += f1\n",
    "mean_f1 /= 3\n",
    "print(\"Mean f1: \", mean_f1)\n",
    "# y_pred = dt.predict(test_x)\n",
    "# test_f1 = f1_score(y_test, y_pred)\n",
    "# print(\"Test f1 score: \", test_f1)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f1:  0.9973249212864211\n"
     ]
    }
   ],
   "source": [
    "mean_f1 = 0\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    kx_train= train_x[train_index]\n",
    "    kx_test = train_x[test_index]\n",
    "    ky_train = train_y[train_index]\n",
    "    ky_test = train_y[test_index]\n",
    "    NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10), random_state=1)\n",
    "    NN.fit(kx_train, ky_train)\n",
    "    y_pred = NN.predict(kx_test)\n",
    "    f1 = f1_score(ky_test, y_pred)         \n",
    "    mean_f1 += f1\n",
    "mean_f1 /= 3\n",
    "print(\"Mean f1: \", mean_f1)\n",
    "# y_pred = dt.predict(test_x)\n",
    "# test_f1 = f1_score(y_test, y_pred)\n",
    "# print(\"Test f1 score: \", test_f1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=4, shuffle = True, random_state=200)\n",
    "# mean_accuracy = 0\n",
    "# # train_x = train_x.values # to convert df to numpy arrays\n",
    "# train_y = y_train.values\n",
    "# best_accuracy = 0\n",
    "# for train_index, test_index in kf.split(train_x):\n",
    "#     kx_train= train_x[train_index]\n",
    "#     kx_test = train_x[test_index]\n",
    "#     ky_train, ky_test = train_y[train_index], train_y[test_index]\n",
    "#     # multi layer perceptron\n",
    "#     NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,1), random_state=1)\n",
    "#     NN.fit(kx_train, ky_train)\n",
    "#     y_pred = NN.predict(kx_test)\n",
    "#     acc = metrics.accuracy_score(ky_test, y_pred)\n",
    "#     print(acc)\n",
    "#     if(acc > best_accuracy):\n",
    "#         best_accuracy = acc\n",
    "#         with open('best_pca_nn.pkl', 'wb') as f:\n",
    "#             pickle.dump(NN, f)            \n",
    "#     mean_accuracy += acc\n",
    "# mean_accuracy /= 4\n",
    "# print(\"Best Validation Accuracy : \", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('best_pca_nn.pkl', 'rb') as f:\n",
    "#     NN = pickle.load(f)\n",
    "# y_pred = NN.predict(test_x)\n",
    "# print(\"Test Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "# print(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=4, shuffle = True, random_state=200)\n",
    "# mean_accuracy = 0\n",
    "# # x_train = x_train.values # to convert df to numpy arrays\n",
    "# train_y = y_train.values\n",
    "# best_accuracy = 0\n",
    "# for train_index, test_index in kf.split(x_train):\n",
    "#     kx_train= x_train[train_index]\n",
    "#     kx_test = x_train[test_index]\n",
    "#     ky_train, ky_test = train_y[train_index], train_y[test_index]\n",
    "#     # multi layer perceptron\n",
    "#     NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(8, 2), random_state=1)\n",
    "#     NN.fit(kx_train, ky_train)\n",
    "#     y_pred = NN.predict(kx_test)\n",
    "#     acc = metrics.accuracy_score(ky_test, y_pred)\n",
    "#     print(acc)\n",
    "#     if(acc > best_accuracy):\n",
    "#         best_accuracy = acc\n",
    "#         with open('best_nn.pkl', 'wb') as f:\n",
    "#             pickle.dump(NN, f)            \n",
    "#     mean_accuracy += acc\n",
    "# mean_accuracy /= 4\n",
    "# print(\"Best Validation Accuracy : \", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #without pca on data mlp\n",
    "# with open('best_nn.pkl', 'rb') as f:\n",
    "#     NN = pickle.load(f)\n",
    "# # NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10, 2), random_state=1)\n",
    "# # NN.fit(x_train, y_train)\n",
    "# y_pred = NN.predict(x_test)\n",
    "# print(\"Validation Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "# print(tn,fp,fn,tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9961464354527938\n",
      "True positive:  628\n",
      "True neative:  406\n",
      "False positive:  1\n",
      "False negative:  3\n",
      "0.9968253968253968\n"
     ]
    }
   ],
   "source": [
    "NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5), random_state=1)\n",
    "NN.fit(train_x, train_y)\n",
    "y_pred = NN.predict(test_x)\n",
    "print(\"Test Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(\"True positive: \", tp)\n",
    "print(\"True neative: \",tn)\n",
    "print(\"False positive: \",fp)\n",
    "print(\"False negative: \", fn)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "print(test_f1)\n",
    "\n",
    "with open('best_nn.pkl', 'wb') as f:\n",
    "    pickle.dump(NN, f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test using random 60 data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 133)\n"
     ]
    }
   ],
   "source": [
    "final_test = df.sample(60)\n",
    "print(final_test.shape)\n",
    "x_final_test = final_test[features]\n",
    "y_final_test = final_test['STATIC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  1.0\n",
      "True positive:  39\n",
      "True neative:  21\n",
      "False positive:  0\n",
      "False negative:  0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#transform x_final_test to pca components\n",
    "final_test_x = pca.transform(x_final_test)\n",
    "#load best model\n",
    "with open('best_nn.pkl', 'rb') as f:\n",
    "    NN = pickle.load(f)\n",
    "#predict using loaded model\n",
    "y_pred = NN.predict(final_test_x)\n",
    "# print(\"Test Accuracy:\",metrics.accuracy_score(y_final_test, y_pred))\n",
    "# tn, fp, fn, tp = confusion_matrix(y_final_test, y_pred).ravel()\n",
    "# print(tn,fp,fn,tp)\n",
    "print(\"Test Accuracy: \",metrics.accuracy_score(y_final_test, y_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_final_test, y_pred).ravel()\n",
    "print(\"True positive: \", tp)\n",
    "print(\"True neative: \",tn)\n",
    "print(\"False positive: \",fp)\n",
    "print(\"False negative: \", fn)\n",
    "final_test_f1 = f1_score(y_final_test, y_pred)\n",
    "print(final_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
